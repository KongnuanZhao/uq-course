{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 11 - Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "+ to do regression using a GP\n",
    "+ to find the hyperparameters of the GP by maximizing the (marginal) likelihood\n",
    "+ to use GP regression for uncertainty propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readings\n",
    "\n",
    "+ Please read [this](http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/pdfs/pdf2903.pdf) OR watch [this video lecture](http://videolectures.net/mlss03_rasmussen_gp/?q=MLSS).\n",
    "\n",
    "+ [Section 5.4 in GP for ML textbook](http://www.gaussianprocess.org/gpml/chapters/RW5.pdf).\n",
    "\n",
    "+ See slides for theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "The purpose of this example is to demonstrate Gaussian process regression. To motivate the need let us introduce a toy uncertainty quantification example:\n",
    "\n",
    "> We have developed an \"amazing code\" that models an extremely important physical phenomenon. The code works with a single input paramete $x$ and responds with a single value $y=f(x)$. A physisis, who is an expert in the field, tells us that $x$ must be somewhere between 0 and 1. Therefore, we treat it as uncertainty and we assign to it a uniform probability density:\n",
    "$$\n",
    "p(x) = \\mathcal{U}(x|0,1).\n",
    "$$\n",
    "Our engineers tell us that it is vitally important to learn about the average behavior of $y$. Furthermore, they believe that a value of $y$ greater than $1.2$ signifies a catastrophic failure. Therefore, we wish to compute:\n",
    "1. the variance of $y$:\n",
    "$$\n",
    "v_y = \\mathbb{V}[f(x)] = \\int\\left(f(x) - \\mathbb{E}[f(x)]\\right)^2p(x)dx,\n",
    "$$\n",
    "2. and the probability of failure:\n",
    "$$\n",
    "p_{\\mbox{fail}} = P[y > 1.2] = \\int\\mathcal{X}_{[1.2,+\\infty)}(f(x))p(x)dx,\n",
    "$$\n",
    "where $\\mathcal{X}_A$ is the characteristic function of the set A, i.e., $\\mathcal{X}_A(x) = 1$ if $x\\in A$ and $\\mathcal{X}_A(x) = 0$ otherwise.\n",
    "Unfortunately, our boss is not very happy with our performance. He is going to shut down the project unless we have an answer in ten days. However, a single simulation takes on day... We can only do 10 simulations! What do we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the \"amazing code\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Here is an amazing code:\n",
    "solver = lambda(x): -np.cos(np.pi * x) + np.sin(4. * np.pi * x)\n",
    "# It accepts just one input parameter that varies between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Learning About GP Regression\n",
    "This demonstrates how do do Gaussian process regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "import cPickle as pickle\n",
    "import GPy\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(1345678)\n",
    "\n",
    "# Select the number of simulations you want to perform:\n",
    "num_sim = 10\n",
    "\n",
    "# Generate the input data (needs to be column matrix)\n",
    "X = np.random.rand(num_sim, 1)\n",
    "\n",
    "# Evaluate our amazing code at these points:\n",
    "Y = solver(X)\n",
    "\n",
    "# Pick a covariance function\n",
    "k = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.)\n",
    "\n",
    "# Construct the GP regression model\n",
    "m = GPy.models.GPRegression(X, Y, k)\n",
    "m.optimize()\n",
    "# That's it. Print some details about the model:\n",
    "print m\n",
    "\n",
    "# Now we would like to make some predictions\n",
    "# Namely, we wish to predict at this dense set of points:\n",
    "X_p = np.linspace(0, 1., 100)[:, None]\n",
    "\n",
    "# We can make predictions as follows\n",
    "Y_p, V_p = m.predict(X_p) # Y_p = mean prediction, V_p = predictive variance\n",
    "# Here is the standard deviation:\n",
    "S_p = np.sqrt(V_p)\n",
    "# Lower predictive bound\n",
    "Y_l = Y_p - 2. * S_p\n",
    "# Upper predictive bound\n",
    "Y_u = Y_p + 2. * S_p\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X_p, Y_p, label='Predictive mean')\n",
    "ax.fill_between(X_p.flatten(), Y_l.flatten(), Y_u.flatten(), alpha=0.25, label='Predictive error bars')\n",
    "ax.plot(X, Y, 'kx', markeredgewidth=2, label='Observed data')\n",
    "\n",
    "# Write the model to a file \n",
    "print '> writing model to file: surrogate.pcl'\n",
    "with open('surrogate.pcl', 'wb') as fd:\n",
    "    pickle.dump(m, fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. The fit looks pretty bad. Why do you think that is? Are our prior assumptions about the parameters of the GP compatible with reality?\n",
    "\n",
    "2. Ok. We know that our code is deterministic but the GP thinks that there is noise there. Let’s fix this. Go to line 40 and type:\n",
    "```\n",
    "   m.likelihood.variance = 0\n",
    "```\n",
    "This tells the GP that the observations have no noise. Rerun the code. Is the fit better?\n",
    "3. The previous question was not supposed to work. Why do you think it failed? It\n",
    "can be fixed by making the variance something small, e.g., make it 1e-6 instead of exactly zero. Rerun the code. Is the fit now any better?\n",
    "4. We are not quite there. Look at line 36. The length scale we are using is 1. Perhaps our function is not that smooth. Try to pick a more reasonable value for the length scale and rerun the code. What do you think is a good value?\n",
    "5. Repeat 3 for the variance parameter of the SE covariance function.\n",
    "6. That’s too painful and not very scientific. The proper way to find the parameters is to maximize the likelihood. Undo the modifications you made so far, go to line 40, and type:\n",
    "```\n",
    "   m.optimize()\n",
    "```\n",
    "This maximizes the marginal likelihood of your model using the BFGS algorithm and honoring any constraints. Rerun the examples. What are the parameters that the algorithm finds? Do they make sense? How do the results look like?\n",
    "7. Based on the results you obtained in 5, we decide to ask our boss for one more\n",
    "day. We believe that doing one more simulation will greatly reduce error in our predictions. At which input point you think we should make this simulation? You can add it by going to line 31 and typing:\n",
    "```\n",
    "   X = np.vstack([X, [[0.7]]])\n",
    "```\n",
    "where, of course, you should replace “0.7” with the point you think is the best. This just appends a new input point to the existing X. Rerun the example. What fit do you get now? \n",
    "8. If you are this fast, try repeating 5-6 with a less smooth covariance function, e.g.,\n",
    "the Matern32. What do you observe? Is the prediction uncertainty larger or smaller?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "Here we use the results of 3.1 to propagate uncertainty. The following code computes the mean of the output of the \"amazing code\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ensure reproducibility\n",
    "np.random.seed(1345678)\n",
    "\n",
    "# Load the model that we constructed in Example 3.1\n",
    "with open('surrogate.pcl', 'rb') as fd:\n",
    "    m = pickle.load(fd)\n",
    "\n",
    "print '> doing uncertainty propagation the following surrogate:'\n",
    "print m\n",
    "\n",
    "# Number of Monte Carlo samples to take\n",
    "num_mc = 10000\n",
    "\n",
    "# Generate random input\n",
    "X = np.random.rand(num_mc, 1)\n",
    "\n",
    "# Evaluate the surrogate output at each one of those points\n",
    "Y = m.predict(X)[0]\n",
    "\n",
    "# For comparison, we also evaluate the real code\n",
    "Y_t = solver(X)\n",
    "\n",
    "# Compute the variance of y\n",
    "var = Y.var()\n",
    "print '> variance y estimate using surrogate:\\t{0:1.4f}'.format(var)\n",
    "# A true Monte Carlo estimate of the variance\n",
    "var_t = Y_t.var()\n",
    "print '> variance y estimate using solver:\\t{0:1.4f}'.format(var_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "1. How accurate are the results we obtain?\n",
    "2. Use the following code to compute our estimate for the probability of failure:\n",
    "```\n",
    "p_fail = np.ones((num_mc, 1))[Y > 1.2].sum() / num_mc\n",
    "```\n",
    "Do the same for real solver and compare the results (just do “print p_fail”, do not bother formatting the output.\n",
    "3. Plot a histogram of our estimate for the probability density of y and compare to\n",
    "the true one. Use this code:\n",
    "```\n",
    "   plt.hist(Y, normed=True, bins=100, color=‘blue’, alpha=0.25)\n",
    "   plt.hist(Y_t, normed=True, bins=100, color=‘red’, alpha=0.25)\n",
    "   plt.show(block=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "Here you have to do the best you can to model the motorcycle data using a GP.\n",
    "We just load the data for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('../data/motor.dat')\n",
    "X = data[:, 0][:, None]   # Makes this a one-column matrix\n",
    "Y = data[:, 1][:, None]   # Same as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. Use a standard GP to fit the data. \n",
    "2. How good is your fit? What seems to be the problem?\n",
    "3. You need a heteroscedastic GP. That is a GP with input-dependent noise. This is an open problem. Nevertheless, there is a simple way to deal with this here. Cluster the data into regions exhibiting different levels of noise and build a different GP on each one of them. How many such regions do you see. Modify the code you have above to do this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
