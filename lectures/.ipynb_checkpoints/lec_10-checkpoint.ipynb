{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10 - Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "% DEFINITIONS\n",
    "\\newcommand{\\bff}{\\mathbf{f}}\n",
    "\\newcommand{\\bm}{\\mathbf{m}}\n",
    "\\newcommand{\\bk}{\\mathbf{k}}\n",
    "\\newcommand{\\bx}{\\mathbf{x}}\n",
    "\\newcommand{\\by}{\\mathbf{y}}\n",
    "\\newcommand{\\bz}{\\mathbf{z}}\n",
    "\\newcommand{\\bA}{\\mathbf{A}}\n",
    "\\newcommand{\\bB}{\\mathbf{B}}\n",
    "\\newcommand{\\bC}{\\mathbf{C}}\n",
    "\\newcommand{\\bD}{\\mathbf{D}}\n",
    "\\newcommand{\\bI}{\\mathbf{I}}\n",
    "\\newcommand{\\bK}{\\mathbf{K}}\n",
    "\\newcommand{\\bL}{\\mathbf{L}}\n",
    "\\newcommand{\\bM}{\\mathbf{M}}\n",
    "\\newcommand{\\bX}{\\mathbf{X}}\n",
    "\\newcommand{\\bY}{\\mathbf{Y}}\n",
    "\\newcommand{\\bTheta}{\\mathbf{\\Theta}}\n",
    "\\newcommand{\\calX}{\\mathcal{X}}\n",
    "\\newcommand{\\bLambda}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\bmu}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\calN}{\\mathcal{N}}\n",
    "\\newcommand{\\calD}{\\mathcal{D}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\E}{\\mathbb{E}}\n",
    "\\newcommand{\\C}{\\mathbb{C}}\n",
    "\\newcommand{\\Rd}{\\R^d}\n",
    "\\newcommand{\\Rdd}{\\R^{d\\times d}}\n",
    "\\newcommand{\\bzero}{\\mathbf{0}}\n",
    "\\newcommand{\\GP}{\\mbox{GP}}\n",
    "% END OF DEFINITIONS\n",
    "$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Quick recap of Gaussian processes (GP). \n",
    "+ Condition a GP on observed measurements \n",
    "+ Training a GP by maximization of the likelihood\n",
    "\n",
    "## Reading\n",
    "\n",
    "+ Please read [this](http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/pdfs/pdf2903.pdf) OR watch [this video lecture](http://videolectures.net/mlss03_rasmussen_gp/?q=MLSS).\n",
    "\n",
    "\n",
    "\n",
    "Recall, that in the previous lecture we discussed the following :\n",
    "\n",
    "+ What is prior knowledge?\n",
    "\n",
    "+ What is a Gaussian process (GP) ?\n",
    "\n",
    "+ What are the properties of the mean and covariance functions of a Gaussian process and what kind of priors can we encode into a GP through the mean and the covariance kernel? \n",
    "\n",
    "+ How do we sample from a GP ?\n",
    "\n",
    "In this lecture, we shall talk about how we develop response functions to approximate a generic black box computer code (say $f(\\cdot)$) in a manner that makes it compatible with our prior beliefs about the model. We do so, by using Bayes' rule and the Gaussian process regression method. Remember that our goal is to be able to propagate uncertainty in the inputs. \n",
    "\n",
    "We saw in the previous lecture that one's prior knowledge about the response can be modeled in terms of a generic GP. Let that prior state of knowledge be represented as follows: \n",
    "\\begin{equation}\n",
    "f(\\cdot) | m(\\cdot), k(\\cdot, \\cdot) \\sim \\GP\\left(f(\\cdot) | m(\\cdot), k(\\cdot, \\cdot) \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where the terms have their usual meaning i.e., $f(\\cdot)$ is a generic response surface, $m(\\cdot)$ is the prior mean function and $k(\\cdot, \\cdot)$ is the covariance kernel parameterized by a set of hyperparameters $\\bTheta$. Specifically, in the case of the squared exponential covariance kernel, $\\bTheta = \\{s, l_1, l_2, \\cdots, l_3\\}$.\n",
    "\n",
    "\n",
    "Now, assume that we make $n$ _measurements_ or _simulations_ at input locations $\\bx_1, \\bx_2, \\cdots, \\bx_n$ such that $\\bx_i \\in \\R^{d}$. The corresponding observed outputs are $y_1, y_2, \\cdots, y_n$, such that $y_i \\in \\R$. We write $\\bX = \\{\\bx_1, \\bx_2, \\cdots, \\bx_n\\}$ and $\\bY=\\{y_1, y_2, \\cdots, y_n\\}$. Abusing mathematical notation slightly, we use the symbol $\\calD$ to denote $\\bX$ and $\\bY$ collectively. We refer to $\\calD$ as the _observed data_. How does the observed data $\\calD$ affect our state of knowledge about the response surface? \n",
    "\n",
    "The answer lies in a straightforward application of Bayes' rule and Kolmogorov's theorem on the existence of random fields. \n",
    "\n",
    "Our new state of knowledge about the response function, conditioned upon the observed data, is another GP which can be expressed as follows: \n",
    "\n",
    "$$\n",
    "f(\\cdot)|\\calD \\sim GP(f(\\cdot)| m^{*} (\\cdot;\\calD) k^{*}(\\cdot, \\cdot; \\calD))\n",
    "$$\n",
    "\n",
    "with mean function: \n",
    "$$\n",
    "m^{*}(\\bx) = m(\\bx) + \\bk(\\bx, \\bX) \\bK(\\bX, \\bX)^{-1}(\\bY-m(\\bX))\n",
    "$$\n",
    "\n",
    "and covariance function:\n",
    "$$\n",
    "k^{*}(\\bx, \\bx') = k(\\bx, \\bx') - \\bk(\\bx, \\bX)\\bK(\\bX, \\bX)^{-1}\\bk(\\bX, \\bx')\n",
    "$$\n",
    "\n",
    "\n",
    "and posterior distribution of the hyperparameters given by : \n",
    "$$\n",
    "p(\\bTheta|\\calD) = \\frac{p(\\calD|\\bTheta)p(\\bTheta)}{p(\\calD)}\n",
    "$$\n",
    "\n",
    "\n",
    "where, \n",
    "$\n",
    "p(\\calD|\\bTheta) = p(\\bY|\\bX, \\bTheta) = \\calN(\\bY|m(\\bX; \\bTheta), k(\\bX, \\bX; \\bTheta))\n",
    "$\n",
    "is called the _likelihood_ of the observed data $\\calD$ and \n",
    "$\n",
    "p(\\calD) = \\int p(\\calD|\\bTheta) p(\\bTheta) d\\bTheta\n",
    "$\n",
    "is called the _evidence_.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
